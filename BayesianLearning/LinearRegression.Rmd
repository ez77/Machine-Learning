---
title: "Linear Regression"
output: html_notebook
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# Linear Model

Linear regression can be established and interpreted from the Bayesian approach.

Recall that in linear regression, we are given target values $y$, data $X$, and the model is
$$y = X \beta + \varepsilon$$
where $\varepsilon$ is the error term.

In classical linear regression, the error term is assumed to have Normal distribution, and so it immediately follows that $y$ is normally distributed with mean $X\beta$, and the variance depends on the error term, i.e. $\sigma^2$. 

The normal model is also what we use in Bayesian regression.

Two main problems need to be adressed:

* inference of $\beta$;

* prediction of $y$ for any new data $X$. 

### Example 1:

Simulated data - we can approximate every function of our real data, normal data are one of the easiest.
$$y= sin(2\pi x)\mathbb{I}(x\leq0)+0.5sin(4\pi x)\mathbb{I}(x>0)$$
Data extraction:.
```{r}
library(ggplot2)
 
# Get data 
X <- (-30:30)/30 #sample points
N <- length(X) 
D <- 10 
var <- 0.15*0.15 
e <- rnorm(N,0,var^0.5) #Assume a normal distribution as variance
EY <- sin(2*pi*X)*(X<=0) + 0.5*sin(4*pi*X)*(X>0) #Sample function evaluated
Y <- sin(2*pi*X)*(X<=0) + 0.5*sin(4*pi*X)*(X>0) + e #Function with sample points
data <- data.frame(X,Y) 
g1 <- ggplot(data=data) + geom_point(mapping=aes(x=X,y=Y)) #plot
g1
```

We also expand features of $x$ (denoted in code as phi_X, under section Construct basis functions). Just as we would expand $x$ into $x^2$, etc., we now expand it into 9 radial basis functions, each one looking like the follows:

$$\phi_1(x) = e^{\frac{||x-\mu_1||^2}{2\sigma^2}}$$
One advantage of radial basis functions is that radial basis functions can fit a variety of curves, including polynomial and sinusoidal.

```{r}
# Construct basis functions 
phi_X <- matrix(0, nrow=N, ncol=D)
phi_X[,1] <- X
mu <- seq(min(X),max(X),length.out=D+1)
mu <- mu[c(-1,-length(mu))]
for(i in 2:D){
 phi_X[,i] <- exp(-(X-mu[i-1])^2/(2*var))
}

# We use non-informative prior
m0 <- matrix(0,D,1)
SN <- solve(t(phi_X)%*%phi_X/var)
mN <- SN%*%t(phi_X)%*%Y/var
Y_hat <- t(mN) %*% t(phi_X)
var_hat <- array(0, N)
for(i in 1:N){
 var_hat[i] <- var + phi_X[i,]%*%SN%*%phi_X[i,]
}

funct <- function(x) sin(2*pi*x)*(x<=0) + 0.5*sin(4*pi*x)*(x>0)

g_bayes <- g1 + 
             geom_line(mapping=aes(x=X,y=Y_hat[1,]),color='#0000FF', size=1) +
             geom_function(fun = funct, col = "red", size=1)


g_bayes_full <- g_bayes + geom_ribbon(mapping=aes(x=X,y=Y_hat[1,],
                  ymin=Y_hat[1,]-1.96*var_hat^0.5,
                  ymax=Y_hat[1,]+1.96*var_hat^0.5, alpha=0.1),
                  fill='#9999FF')
g_bayes_full
```

The blue line is the expected value of the predictive distribution at each point x, and the light blue region refers to regions within two standard deviations. Red line is the true function of y. Dots are data randomly generated from the given function with normal noise.

### Example 2:

Real data:

Lists estimates of the percentage of body fat determined by underwater weighing and various body circumference measurements for 252 men. Accurate measurement of body fat is inconvenient/costly and it is desirable to have easy methods of estimating body fat that are not inconvenient/costly.

```{r}
library(BAS)
rm(list=ls())
data(bodyfat)
#?bodyfat
summary(bodyfat)
```


#### Frequentist OLS linear regression

This regression model can be formulated as
$$y_i = \alpha + \beta x_i + \epsilon_i$$
for $i=1, \ldots, 252$.

```{r}
hist(bodyfat$Bodyfat, main="Histogram of bodyfat")
```


```{r, warning=FALSE}
bodyfat.lm = lm(Bodyfat ~ Abdomen, data = bodyfat)

summary(bodyfat.lm)

# Extract coefficients
beta = coef(bodyfat.lm)

# Visualize regression line on the scatter plot
library(ggplot2)

ggplot(data = bodyfat, aes(x = Abdomen, y = Bodyfat)) +
  geom_point(color = "blue") +
  geom_abline(intercept = beta[1], slope = beta[2], size = 1) +
  xlab("abdomen circumference (cm)") 

```
#### Bayesian linear regression:

The Bayesian model starts with the same model as the classical frequentist approach:
$$y_i = \alpha + \beta x_i + \epsilon_i$$
Our goal is to update the distributions of the unknown parameters  $\alpha$, $\beta$ and $\sigma^2$ based on the data ($x_1, \ldots, x_n$ and $y_1, \ldots, y_n$) where $n$ is the number of observations.

We assume noninformative priors,
$$p(\alpha, \beta| \sigma^2) \propto 1$$
$$p(\sigma^2) \propto \frac{1}{\sigma^2}$$

Then we apply the Bayes rule to derive the joint posterior distribution after observing data $y_1, \ldots, y_n$.

Posterior distribution of $\sigma^2$ is an Inverse Gamma, so we can write
$$\frac{1}{\sigma^2}\mid data \sim Gamma\Big(\frac{n-2}{2}, \frac{SSE}{2}\Big)$$
where SSE is um of squares of errors.
Furthermore, the posterior distribution of $\beta$ conditioning on $\sigma^2$ is
$$\beta|\sigma^2, data \sim N\Big(\hat{\beta}, \frac{\sigma^2}{S_{xx}}\Big)$$
where $S_{xx} = \sum_{i=1}^n (x_i - \bar{x})^2$ and the posterior distribution of $\alpha$ conditioning on $\sigma^2$ is
$$\alpha|\sigma^2, data \sim N\Big(\hat{\alpha}, \sigma^2 \big(\frac{1}{n}+\frac{\bar{x}^2}{S_{xx}}\big)\Big)$$

The Bayesian posterior distribution results of $\alpha$ and $\beta$ show that under the reference prior, the posterior credible intervals are in fact numerically equivalent to the confidence intervals from the classical frequentist OLS analysis. This provides a baseline analysis for other Bayesian analyses with other informative prior distributions or perhaps other “objective” prior distributions, such as the Cauchy distribution. (Cauchy distribution is the Student’s $t$ prior with 1 degree of freedom.)
Since the credible intervals are numerically the same as the confidence intervals, we can use the lm function to obtain the OLS estimates and construct the credible intervals of alpha and beta

```{r}
output = summary(bodyfat.lm)$coef[, 1:2]
output
```

The confint function provides 95% confidence intervals. Under the reference prior, 
they are equivalent to the 95% credible intervals. The code below extracts 
them and relabels the output as the Bayesian results.

```{r}
out = cbind(output, confint(bodyfat.lm))
colnames(out) = c("posterior mean", "posterior std", "2.5", "97.5")
round(out, 2)

```

These intervals coincide with the confidence intervals from the frequentist approach. 

The primary difference is the interpretation. For example, based on the data, 
we believe that there is 95% chance that body fat will increase by 5.75% up to 6.88% 
for every additional 10 centimeter increase in the waist circumference.

```{r}
# Credible Intervals for the Mean mu and the prediction of y_n+1:
library(ggplot2)

# Construct current prediction
alpha = bodyfat.lm$coefficients[1]
beta = bodyfat.lm$coefficients[2]
new_x = seq(min(bodyfat$Abdomen), max(bodyfat$Abdomen), 
            length.out = 100)

y_hat = alpha + beta * new_x

# Get lower and upper bounds for mean
ymean = data.frame(predict(bodyfat.lm,
                           newdata = data.frame(Abdomen = new_x),
                           interval = "confidence",
                           level = 0.95))

# Get lower and upper bounds for prediction
ypred = data.frame(predict(bodyfat.lm,
                           newdata = data.frame(Abdomen = new_x),
                           interval = "prediction",
                           level = 0.95))

output = data.frame(x = new_x, y_hat = y_hat, ymean_lwr = ymean$lwr, ymean_upr = ymean$upr, 
                    ypred_lwr = ypred$lwr, ypred_upr = ypred$upr)

# Extract potential outlier data point
outlier = data.frame(x = bodyfat$Abdomen[39], y = bodyfat$Bodyfat[39])

# Scatter plot of original
plot1 = ggplot(data = bodyfat, aes(x = Abdomen, y = Bodyfat)) + geom_point(color = "blue")

# Add bounds of mean and prediction
plot2 = plot1 + 
  geom_line(data = output, aes(x = new_x, y = y_hat, color = "first"), lty = 1) +
  geom_line(data = output, aes(x = new_x, y = ymean_lwr, lty = "second")) +
  geom_line(data = output, aes(x = new_x, y = ymean_upr, lty = "second")) +
  geom_line(data = output, aes(x = new_x, y = ypred_upr, lty = "third")) +
  geom_line(data = output, aes(x = new_x, y = ypred_lwr, lty = "third")) + 
  scale_colour_manual(values = c("orange"), labels = "Posterior mean", name = "") + 
  scale_linetype_manual(values = c(2, 3), labels = c("95% CI for mean", "95% CI for predictions")
                        , name = "") + 
  theme_bw() + 
  theme(legend.position = c(1, 0), legend.justification = c(1.5, 0))

plot2
```


Outlier
```{r}
# Identify potential outlier
plot2 + geom_point(data = outlier, aes(x = x, y = y), color = "orange", pch = 1, cex = 6)

# Note in the above plot, the legend "CI" can mean either confidence interval or credible 
# interval. The difference comes down to the interpretation. For example, 
# the prediction at the same abdominal circumference as in Case 39 is

pred.39 = predict(bodyfat.lm, newdata = bodyfat[39, ], interval = "prediction", level = 0.95)
out = cbind(bodyfat[39,]$Abdomen, pred.39)
colnames(out) = c("abdomen", "prediction", "lower", "upper")
out
```


Based on the data, a Bayesian would expect that a man with waist circumference of 148.1 
centermeters should have bodyfat of 54.216% with 95% chance that it is between 44.097% 
and 64.335%.
 
While we expect the majority of the data will be within the prediction intervals 
(the short dashed grey lines), Case 39 seems to be well below the interval.


